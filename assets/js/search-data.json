{
  
    
        "post0": {
            "title": "A tutorial on image classification using fastaiv2",
            "content": "Introduction . classifying objects is one of the primary tasks of Deep learning in computer vision. If this happens at image level it is called image classification and at pixel level it is called segmentation. Image (object) classification is the core for any analytics work you do on images. In this blogpost, lets see how image classification is done effectively using fastaiv2. . Dataset and DataBlock . The fundamental thing which we need for any task is dataset. In fastaiv2, dataset is represented in many formats. . from fastai2.vision.all import * . . Note: Fastaiv2 has several datasets apis which we can download simply using the below command . path = untar_data(URLs.PETS) files = get_image_files(path) # get all the image files . There are several ways in which we can create a dataloader using Fastaiv2 and using DataBlock we can define in the following way. . create a func (label_func) which extracts class name from the dataset. dsets.vocab shows the classes of the dataset | get_image_files extracts images from path provided at dblocks.datasets. We can also use FileGetter(extensions=&quot;.jpg&quot;) as an input to get_items if u want to specify the params. | There are several splitters available in the fastaiv2 including RandomSplitter: takes fraction of validation data as input | TrainTestSplitter: A wrapper to sklearn train_test_split function | IndexSplitter: takes the index of valid_idx as input | GrandparentSplitter: used when your train and val datasets are present in different folders | FuncSplitter: When you write your own function to divide the dataset into train and validation | FileSplitter: image names present in a file.txt as valid | RandomSubsetSplitter: give fractions to both train and val | ColSplitter: split the dataset based on a column | . | blocks: Kind of outputs we need, here since we are using images and need category as output we are using ImageBlock and CategoyBlock. Use ImageBlock(cls=PILImageBW) when u need a Black &amp; white image as input to network | Use MultiCategory block when u are training a multi-class classification. | TODO | . | . def label_func(fname): return &quot;cat&quot; if fname.name[0].isupper() else &quot;dog&quot; . dblock = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = label_func, splitter = RandomSplitter(), item_tfms = Resize(224)) dsets = dblock.datasets(path) . . Tip: dsets is similar to dataset in Pytorch. We can write our own dsets if fastai DataBlock doesn&#8217;t work for us, please make sure we implement show method to visualize an input. dsets is a dataset. if we need a dataloader, we need to call dataloader method. dataloader method is very similar to dataloader in Pytorch. It has arguments like batch_size(bs), shuffle, pin_memory, drop_last. . Tip: Fastai uses delegates for auto-completion and to see what arguments are present, do shift+tab to check all the arguments on a function. . Important: Fastai dataloader comes with a show batch functionality, which will help you visualize a batch of images. . dls = dblock.dataloaders(path/&quot;images&quot;, bs=32) . . Warning: show_batch doesn&#8217;t work if all the images are not of the same size. we need to work define item_tfms to resize all the images to the same size. . dls.show_batch() . . Important: In vision we also have ImageDataLoaders where we can initialize a dataloader in a single line depending on the type of input . ImageDataLoaders . There are several different image data loaders depending on the type of input data. Here since our data is coming from paths | . fnames = get_image_files(path/&quot;images&quot;) dls = ImageDataLoaders.from_path_func(path/&quot;images&quot;, fnames, label_func, valid_pct=0.2, item_tfms=Resize(224)) . Transformations and Agumentations. . Deep learning architecutures contain large number of parameters (in millions) and it is very hard to achieve generalization with limited datasets. even the large scale imagenet dataset also contain 1.3 million images. Thanks to the image agumentations, we can randomly flip, rotate, resize, pad, crop and do many other image agumentations and create an artificially large dataset which will help us in training deep learning models. . In fastaiv2, we have two different types transformations . item transforms | batch transforms | . Traditionally everything is performed as item transform but fastai cleverly divided transformations/agumentations into two different types. one which is applied on images and one which is applied on tensors. one which are applied on images are called item transforms and these are performed on cpu irrespective of GPU availability. batch transforms are applied on tensors and can be done GPU like normalization and standardization. we can have a look at different transforms implementation here . dls = ImageDataLoaders.from_path_func(path/&quot;images&quot;, fnames, label_func, valid_pct=0.2, item_tfms=[FlipItem(0.5), Resize(224, method=&quot;pad&quot;)], batch_tfms=[Normalize.from_stats(*imagenet_stats)]) . . Tip: use dls.one_batch() gives one batch of imgtensor and labels. use this to test/check loss functions, network and dataloader ouputs. TODO: . writing your own item_tfms or batch_tfms | . Networks . We will specifically talk about fastaiv2 computer vision architectures. fastaiv2.vision.learner has create_head, create_body and create_cnn_model. specifically a network is divided into two parts: body and head. later we will see in learner how this will be useful. . encoder = create_body(resnet18, n_in=3, pretrained=False) head = create_head(nf=512, n_out=2, concat_pool=False) model = nn.Sequential(encoder, head) . . Tip: We can directly call create_cnn_learner and define the network in one single line. The nf feature is directly computed in this case. . model = create_cnn_model(resnet18, n_out=2, pretrained=True, n_in=3, custom_head=None) . . Important: resnet18 is a function which takes pretrained as an argument, if you are defining your own architecture, make sure that it takes this as an argument. . from segmentation_models_pytorch.encoders import get_encoder def SimpleArch(encoder: str= &quot;resnet18&quot;): def model(pretrained: Union[str]=None): return get_encoder(encoder, in_channels=3, depth=5, weights=pretrained) return model . . Note: we can define custom heads to the create_cnn_model. . @delegates(create_head, but=&quot;nf, n_out&quot;) class FastaiDecoder(Module): def __init__(self, name, nf, n_out, **kwargs): store_attr(self, &quot;name, nf, n_out&quot;) setattr(self, f&quot;{self.name}_decoder&quot;, create_head(nf, n_out, **kwargs)) def forward(self, x): x = getattr(self, f&quot;{self.name}_decoder&quot;)(x) return x . . Tip: FastaiDecoder is inherited from fastaiv2 Module and not torch nn module. when using fastaiv2 Module we need not use super for inheritance. . Tip: If we want to train a cnn arch and head, we can directly define them inside cnn_learner and no need to use create_head and create_body functions. Use create_head and create_body when we are using Learner API. As defined in fastai docs, a learner is a Trainer for model using data to minimize loss_func with optimizer opt_func. . learn = cnn_learner(dls, resnet18, pretrained=True, n_out=2, metics=[accuracy]) . learn.lr_find() . learn.fit_one_cycle(3) . There are lot of parameters in cnn_learner which needs attention. we will go through each of this below. I will divide the params into two parts, . obvious params . dls: dataloader. | arch: architecuture body. This is a function which takes argument as pretrained | loss_func: loss function to use. We will have separate section for loss function. Here we will use CrossEntropyLossFalt. . Tip: we need not mention a loss function, depending on the data input and output, cnn_learner chooses a loss function automatically for u. check learn.loss_func to see which loss function u are using for a given problem statement. - pretrained=True: type depends on the type of input arch takes, | cut=None: if we want to cut the arch. Mostly keep it None. | . TBD . splitter=None, y_range=None, config=None, n_out=None, normalize=True, opt_func=&lt;function Adam at 0x7fab5cb45c20&gt;, lr=0.001 cbs=None metrics=None path=None model_dir=&#39;models&#39; wd=None wd_bn_bias=False train_bn=True moms=(0.95, 0.85, 0.95) . work in progress . How to define your own loss function? (focal loss) | How to define your own metric? (auc_roc_score) | write custom callbacks to save model body and head separately. (SAVEMODELHEAD) | some useful tips for training classifiers | use mix precision training | training on a TPU | . Resources . https://www.kaggle.com/jhoward/fastai-v2-pipeline-tutorial | https://docs.fast.ai/basic_train.html#Discriminative-layer-training | .",
            "url": "https://prakashjayy.github.io/carbon2silicon/fastai/python/deep-learning/2020/07/04/fastaiv2_image_classification_101.html",
            "relUrl": "/fastai/python/deep-learning/2020/07/04/fastaiv2_image_classification_101.html",
            "date": " • Jul 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "A tutorial on understanding fastai apis",
            "content": "Useful decorators . Delegates . usually when we write a function with kwargs, it is difficult to get tab completion, so fastcore has implemented delegates function which does this for you. | It is extensively used in fastai2. | . import inspect from fastcore.foundation import delegates def create_web_page(title, category=&quot;General&quot;, date=None, author=&quot;Jeremy&quot;): ... @delegates(create_web_page) def create_product_page(title, price, cost, **kwargs): ... def create_product_page2(title, price, cost, **kwargs): ... print(inspect.signature(create_product_page)) print(inspect.signature(create_product_page2)) . (title, price, cost, category=&#39;General&#39;, date=None, author=&#39;Jeremy&#39;) (title, price, cost, **kwargs) . @patch, @patch_to and @patch_property . @patch and @patch_to tries to add methods to the already defined classes. We have used patch to add pad_resize to Image module. | @patch_property adds attributes to already defined classes. suppose you have PIL.Image which doesn&#39;t have shape property. You want add this method to the PIL.Image, how can we do this? | . from PIL import Image from fastcore.foundation import patch_property, patch @patch_property def shape(x: Image.Image): return (x.size[0], x.size[1]) @patch def pad_resize(self: Image, size: tuple): &quot;Does padding and resize based on h/w&quot; return self . f = Image.open(&quot;../images/favicon.ico&quot;) print(f.shape) Image.pad_resize(f, size=(20, 20)) # . (25, 35) . @typedispatch . whenever we want to make a function work depending on the input type we can use typedispatch. say for example we want to add two numbers only if their types are both the same. | It can be heavily used in augmentations in DL vision, where we implement different functions for say Resize of mask and Resize of image. Using typedispatch, makes us call only one function called resize, but appropriate resize happens after checking the type. | . from fastcore.dispatch import typedispatch @typedispatch def f(x: int, y: int): return x+y @typedispatch def f(x, y): return &quot;It doesn&#39;t work like this&quot; . print(f(1, 2)) print(f(1, 2.)) . 3 It doesn&#39;t work like this . @typedispatch def f2(x: int): return x . f2(9.0) . 9.0 . @Transform . Transform uses encodes, decodes and setup to make transformations to input objects. . encodes: encodes the input, say resize the image from (H, W) to (h, w) | decodes: decodes the output of encodes to inputs of encodes. resize back from (h,w) to (H, W) | setups: can be used to setup attributes when the function is defined | . Note: . if a tuple with different types are sent, only those types matching with encodes are processed. | input types are maintained for output also. if we don&#39;t want this, we can use -&gt; None | . from fastcore.transform import Transform . class A(Transform): def encodes(self, x:(int,float)): return x/2 def encodes(self, x:(str,list)): return str(x)+&#39;1&#39; def decodes(self, x: (int, float)): return x*2 . f = A() print(f(3)) print(f(&quot;a&quot;)) print(f([1, 2])) print(f.decode(1.5)) print(f.decode(&quot;a1&quot;)) print(f((2, &quot;a&quot;))) print(f.decode((2, &quot;a&quot;))) . 1.5 a1 [1, 2]1 3.0 a1 (1.0, &#39;a1&#39;) (4, &#39;a&#39;) . Useful functions . Chunks . converting a list of lists to list for calculating length or passing it to dataloaders is expensive. fastai chunks makes this simple. . from fastai2.torch_core import Chunks . a = [[1, 2], [3, 4]] b = Chunks(a) . print(b.lens) print(b.chunks) print(b.cumlens) print(b.totlen) . (#2) [2,2] [[1, 2], [3, 4]] [0 2 4] 4 . Important points . use fastai2.torch_core.Module instead of torch.nn.Module so that we need not use super | . This is under active development. | .",
            "url": "https://prakashjayy.github.io/carbon2silicon/fastai/python/deep-learning/2020/07/01/fastai_deep_learning_101.html",
            "relUrl": "/fastai/python/deep-learning/2020/07/01/fastai_deep_learning_101.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Probability distributions in bayesian modelling",
            "content": "Introduction . Probability distributions are one of the core concepts of bayesian modelling. Any problem begins with thinking about these distribution. Prior, likelikhood, posterior in bayesian modelling are defined as probability distributions. This blog post is not about listing down all the probability distributions instead it aims is to make the reader understand about a probability distribution and how/why/when it is used in bayesian modelling. The blogpost divided into the following sections . import matplotlib.pyplot as plt plt.style.use(&quot;bmh&quot;) ## use bayesian modeling pymc3 book style import matplotlib import numpy as np import scipy.stats as sps import arviz as az . Simple Problem . Suppose you and your friends are playing coin toss game, for every head, you will get $1 and for every tail you will lose $1. Suppose this is a fair coin and you play enough times, you are sure of not loosing or gaining any money as the probability of heads or tails is 1/2. But, after playing a few games you observed that you are loosing money and now you feel that the coin is baised. You want to now calcuate P(coin baised or not/given data). we will assume $ theta$ to be whether the coin baised or not and y to be the given data. so we want to calculate $P( theta/y)$. . so using bayes theorm, $$ P( theta/y) = (p( theta) * p(y/ theta) ) / p(y) $$ . where . $P( theta)$ - prior $P(y/ theta)$ - likelihood $P(y)$ - normalizing constant $P( theta/y)$ - posterior $P( theta, y)$ = likelihood * prior = joint distribution . Understanding likelihood - binomial distribution . For probability of success in N trails we generally use binomial distribution. $$ P(y/ theta, N) = binom{n}{y} * ( theta)^{y} * ( theta)^{N-y} $$ | . Now suppose we have intialized an experiment and observed various values of success. Let that be . y = [10, 20, 50, 100, 500] N = [50, 100, 200, 300, 1000] . since binomial distribution is a discrete probability distribution, we calculate the likelihood by discretising the probability distribution . y = [10, 20, 50, 100, 500] N = [50, 100, 200, 300, 1000] theta = np.linspace(0, 1, 100)[:, None] pmf = sps.binom.pmf(y, N, theta) . fig, ax = plt.subplots(figsize=(8,3.5)) ax.set(title=&#39;Binomial Distribution n $p(y|theta, N)$&#39;) ax.plot(theta, pmf) labels = [f&quot;y={i}, N={j}&quot; for i, j in zip(y, N)] plt.legend(title=&quot;$likelihoods$&quot;, labels=labels); plt.show() . The above figure is the likelihood function for various experiments we have done. The concept of MLE (maximum likelihood estimation) came from here and is quite popular in frequentist approach. This is simply calculated as the x value at which maximum y (likelihood) value is obtained. . Note that likelihood is not a valid probability distribution, the area under the curve is not equal to 1. . mle = theta[pmf.argmax(0)].reshape(-1) fig, ax = plt.subplots(figsize=(10,3.5)) ax.set(title=&#39;MLE estimation binom n $p(y|theta, N)$&#39;) ax.bar(labels, mle) for group in range(len(mle)): ax.text(labels[group], mle[group]+0.015, np.round(mle[group], 2), fontsize=10, verticalalignment=&quot;center&quot;) plt.show() . We can use the maximum likelihood estimate to get the point estimates of the $ theta$, but we donot get the credible intervals for our estimate. one way in which frequentists approach this problem is by randomly repeating the same experiment several times and noting down $ theta$ value each time, these are called confidence intervals. Here we will approach this problem using bayesian approach . Understanding prior - Beta distribution . priors are beliefs on our parameter before looking at the data. Given enough data, the posterior tends to be the same irrespective of the prior we choose. some people prefer flat priors (uniform distribution) so that we are not baised. Some people choose priors based on their experience. Some use posterior of one problem as prior to another problem statement. Here in this case we will use beta distribution . It is a conjucate prior to binomial distribution (we will understand this below) | Beta distribution adopts several shapes like uniform distribution, gaussian distribution and U-like distribution. | It is always between [0, 1]. | . $$ P( theta)= 1/B(a, b) * x^{a-1} * (1-x)^{b-1} $$where $$ B(a, b) = Gamma{(a)} Gamma{(b)} / Gamma{a+b} $$ is a normalizing constant . theta = np.linspace(0, 1, 100)[:, None] uniform_beta = sps.beta.pdf(theta, a=1, b=1) gaussian_beta = sps.beta.pdf(theta, a=2, b=5) ushape_beta = sps.beta.pdf(theta, a=2, b=2) fig, ax = plt.subplots(figsize=(8,3.5)) ax.set(title=&#39;Beta distribution&#39;) ax.plot(theta, uniform_beta) ax.plot(theta, gaussian_beta) ax.plot(theta, ushape_beta) labels = [&quot;uniform(1, 1)&quot;, &quot;gaussian(2, 5)&quot;, &quot;ushape(2, 2)&quot;] ax.legend(title=&quot;$shapes(a, b)$&quot;, labels=labels); plt.show() plt.show() . Understanding conjucate priors. . The posterior distribution of $ theta$ is product of prior and likelihood divided by the normalizing constant. we can write that as $$ P( theta/y) = P( theta,y) / P(y) $$ where, $$ P( theta, y) = P( theta) * P(y/ theta) $$ . If we assume P($ theta$) to be a continous probability distribution, the normalizing constant is equal to $$ P(y) = int_0^1 P( theta) * P(y/ theta) d theta $$ . sometimes calculating this integral is difficult as it involves some complex understanding of calculus. some integrals are almost difficult to solve as it involves heirarical distributions, but what if we choose a prior when multiplied by likelihood, falls under the same probability distribution as prior. We will calculate the integral part below analytically below. In our example, we have choosen prior to be beta distribution and likelihood to be binomail distribution . $$ P( theta) * P(y/ theta) = (1/B(a, b) * theta^{a-1} * (1- theta)^{b-1} ) * binom{n}{y} * ( theta)^{y} * ( theta)^{N-y} $$re-ordering we get . $$ = (1/B(a, b)) * binom{n}{y} * theta^{a+y-1} * theta^{b+N-y-1} $$We will consider $ gamma$ to be the constant, so . $$ (1/B(a, b)) * binom{n}{y} = gamma $$operating on constants we get $$ = gamma * theta^{a+y-1} * theta^{b+N-y-1} $$ . Lets call this equation (1) $$ P( theta) * P(y/ theta) = gamma * theta^{a+y-1} * theta^{b+N-y-1} $$ . To covert this into a valid probability distribution (beta), we need to multiply and divide by B(a+y, b+N-y) . $$ = [ gamma * B(a+y, b+N-y)] * [1/B(a+y, b+N-y) * theta^{a+y-1} * theta^{b+N-y-1}] $$Now, if we integrate this equation, we get $$ P(y) = int_0^1 P( theta) * P(y/ theta) d theta = gamma * B(a+y, b+N-y) * int_0^1 1/B(a+y, b+N-y) * theta^{a+y-1} * theta^{b+N-y-1} d theta $$ . The integral part is beta distribution and the area under the curve for beta distribution is 1, so we can rewrite the equation as . So equation 2 $$ P(y) = gamma * B(a+y, b+N-y) * 1 $$ . Now according to baye&#39;s theorm . $$ P( theta/y) = P( theta,y) / P(y) $$From equation 1 and 2, we get . $$ P( theta/y) = gamma * theta^{a+y-1} * theta^{b+N-y-1} / gamma * B(a+y, b+N-y) $$The $ gamma$ cancels out and we get, $$ P( theta/y) = theta^{a+y-1} * theta^{b+N-y-1} / B(a+y, b+N-y) $$ . The above equation is nothing but a beta distribution with change in shape parameters from a to a+y and b to b+N-y. . $$ = P(a+y, b+N-y) $$where a and b are shapes of our prior beta distribution. . so, prior beta distribution with shape parameters a and b when multiplied by posterior binomial distribution of y success in N trails is equal to posterior beta distribution with shape parameters a+y and b+N-y. . So inorder to make our calculations, we can choose prior probabilities which have these nice properties with different likelihood. . Lets take an example to solve this, We will take random shape parameters of a=2 and b=2. We will assume 3(y) successes in 10 (N)-trails. The posterior distribution can be calculated as follows . x = np.linspace(0, 1, 200) # priors prior_a = 2 prior_b = 2 prior = sps.beta.pdf(x, a=prior_a, b=prior_b) # likelihood N = 10 y=3 likelihood = sps.binom.pmf(y, N, x) # posterior posterior_a = prior_a+y posterior_b = prior_b + N - y posterior = sps.beta.pdf(x, a=posterior_a, b=posterior_b) . fig, ax = plt.subplots(figsize=(15,3.5), ncols=3, nrows=1) ax.flat[0].set(title=&#39;beta prior&#39;) ax.flat[0].plot(x, prior) ax.flat[0].fill_between(x, 0, prior, alpha=0.5) ax.flat[1].set(title=&#39;binomial likelihood&#39;) ax.flat[1].plot(x, likelihood, color=&quot;#8B0000&quot;) ax.flat[1].fill_between(x, 0, likelihood, alpha=0.5, color=&quot;#8B0000&quot;) ax.flat[2].set(title=&#39;posterior&#39;) ax.flat[2].plot(x, posterior, color=&quot;#FF8C00&quot;) ax.flat[2].fill_between(x, 0, posterior, alpha=0.5, color=&quot;#FF8C00&quot;) #ax.legend(title=&quot;&quot;, labels=[&#39;prior&#39;, &quot;likelihood&quot;, &quot;posterior&quot;]); plt.show() . credible intervals . we can caluclate the credible intervals of any posterior distribution using arviz package . posterior_samples = sps.beta.rvs(posterior_a, posterior_b, size=1000) az.hpd(posterior_samples, credible_interval=0.95) . array([0.12988628, 0.59007809]) . az.plot_posterior({&#39;θ&#39;: posterior_samples}, credible_interval=0.95) plt.show() . Some points to Note . We see that the credible intervals are very large 0.13-0.61, these is because the data is very less (N=10). If we increase the data, the spread of credible intevals tend to reduce. | Try to experiment with different shape parameters of prior distribution, we will get different posterior credible intevals. | As we increase the data, the output of the posterior will be same irrespective of the shape parameters of the prior. | . Using PyMC3 . We will use pymc3 to solve the same problem . import pymc3 as pm . N = 10 np.random.seed(6) success = 3 . basic_model = pm.Model() with basic_model: # Priors for unknown model parameters phi = pm.Beta(&#39;phi&#39;, alpha=2, beta=2) # Likelihood (sampling distribution) of observations Y_obs = pm.Binomial(&#39;Y_obs&#39;, n=N, p=phi, observed=3) . In the above we have defined the model using PyMC3. we will now use randomwalk metropolis hastings algorithm to calculate the posterior. . with basic_model: # draw 500 posterior samples trace = pm.sample(1000, step=pm.Metropolis()) . Multiprocess sampling (4 chains in 4 jobs) Metropolis: [phi] Sampling 4 chains, 0 divergences: 100%|██████████| 6000/6000 [00:00&lt;00:00, 9269.52draws/s] The number of effective samples is smaller than 25% for some parameters. . pm.traceplot(trace); . pm.summary(trace).round(3) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . phi 0.355 | 0.119 | 0.147 | 0.579 | 0.004 | 0.003 | 994.0 | 994.0 | 996.0 | 1127.0 | 1.01 | . I will discuss about traceplot, summary statistics in a separate blog post where we discuss about the random walk metropolis hastings algorithm | We can see that the mean is around ~0.35 with credible intervals [0.147, 0.579], similar to our analytical approach we have taken at the beginning. | The key takeaways from here is that without using calculas to calculate the integral, we used pymc3 randomwalk metropolis sampling alogrithm to estimate the $ theta$. | Conjucate prior is a convinent way to calculate posterior analytically but a lot of times we are usually restricted by the usuage of priors. choosing priors from conjucate will be overally restrictive. Finally, we choose priors based on our beliefs but not because it is a conjucate to some likelihoods. In these cases PyMC3 will help us a lot with different sampling methods like metropolis hasting, NUTS, Gibbs etc. | . Thank you. .",
            "url": "https://prakashjayy.github.io/carbon2silicon/beta%20distribution/binomial%20distribution/pymc3/random-walk-metropolis/conjucate-prior/bayesian%20modelling/2020/05/29/Probability-distributions-in-bayesian-modelling.html",
            "relUrl": "/beta%20distribution/binomial%20distribution/pymc3/random-walk-metropolis/conjucate-prior/bayesian%20modelling/2020/05/29/Probability-distributions-in-bayesian-modelling.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding Monte carlo Simulation",
            "content": "Introduction . Monte carlo estimation refers to simulating hypothetical draws from a probablistic distribution in order to calculate important quantities of that distribution like mean, variance, quantiles etc. In this blog we will look at how to calculate these estimations for any given distribution using python scientific packages like scipy, numpy and matplotlib. . Why Monte carlo? . In general, mathematically we can derive mean, std and other statistics for any standard distribution. For example the mean of any distribution is obtained using $$ E( theta) = int_0^ infty theta P( theta) d theta $$ . sometimes calculating this integral is difficult as it involves some complex understanding of calculus. some integrals are almost difficult to solve when it involves heirarical distributions. In these cases monte carlo simulations will help us estimate the required statistics with a margin of error. . In this blog, we will look into two different examples. . Gamma distribution and estimation of mean using monte carlo simulation | A binomial distribution of N (10) trails where the probability of success follows a beta distribution (2, 2) | . import matplotlib.pyplot as plt plt.style.use(&quot;bmh&quot;) ## use bayesian modeling pymc3 book style import matplotlib import numpy as np import scipy.stats as sps . Example 1 . Lets try to estimate the mean of a Gamma distibution and compare it with actual mean of a gamma distribution. Mean of gamma distribution is $ alpha/ beta$ . alpha = [0.5, 1.0, 1.5, 2.0, 2.5] beta = 0.5 # Probability density function x axis k = np.linspace(0, 20, 100)[:, None] # calculating gamma function at alpha and beta (scale=1/beta) pdf = sps.gamma.pdf(k, alpha, scale=1/beta) print(pdf.shape) . (100, 5) . fig, ax = plt.subplots(figsize=(8,3.5)) ax.set(title=&#39;Gamma Distribution n $p(k|alpha, beta)$&#39;) plt.plot(k, pdf) plt.legend(title=&quot;$alpha$&quot;, labels=alpha); . The mean of gamma distribution under these parameters is given by | . fig, ax = plt.subplots(figsize=(8, 3.5)) ax.set(title=&quot;Gamma distribution mean n $p(k|alpha, beta)$&quot;) x = [f&quot;alpha={i}&quot; for i in alpha] y = [i/beta for i in alpha] plt.bar(x, y) for group in range(len(x)): ax.text(x[group], y[group]+0.25, y[group], fontsize=10, verticalalignment=&quot;center&quot;) . Lets use monte carlo estimation for calculating mean each scenerio | . def simulate(alpha, beta, n): return [sps.gamma.rvs(i, scale=1/beta, size=n, random_state=20).mean() for i in alpha] counts = [2, 20, 200, 2000, 20000, 200000] values = [simulate(alpha, beta, n) for n in counts] fig, ax = plt.subplots(figsize=(8*2, 3.5*3), nrows=3, ncols=2) for num, alp in enumerate(values): ax.flat[num].set(title=f&quot;simulations = {counts[num]}&quot;) x = [f&quot;alpha={i}&quot; for i in alpha] ax.flat[num].bar(x, alp) for group in range(len(x)): ax.flat[num].text(x[group], alp[group]+0.25, np.round(alp[group], 2), fontsize=10, verticalalignment=&quot;center&quot;) . In the above figure, we can conclude that, drawing large samples (Law of large numbers), the estimated mean will be almost equal to actual mean of the standard distrbution. By central limit theorm, this estitmated mean follows a normal distribution with mean(as shown above) and standard error. | . lets compare the distributions of sampled vs actual gamma distribution . def simulate(alpha, beta, n): return sps.gamma.rvs(alpha, scale=1/beta, size=n, random_state=20) alpha = 1.5 beta = 0.5 counts = [200, 2000, 20000, 200000] values = [simulate(alpha, beta, n) for n in counts] k = np.linspace(0, 20, 100)[:, None] pdf = sps.gamma.pdf(k, alpha, scale=1/beta) fig, ax = plt.subplots(figsize=(8*2, 3.5*2), nrows=2, ncols=2) for num, alp in enumerate(values): ax.flat[num].set(title=f&quot;simulations = {counts[num]}&quot;) new_range = np.arange(0, np.int64(alp.max())+1) p_estimated = np.histogram(alp, bins=len(new_range))[0]/alp.shape[0] ax.flat[num].plot(new_range, p_estimated) ax.flat[num].plot(k, pdf) ax.flat[num].legend(title=&quot;shit&quot;, labels=[&quot;estimated&quot;, &quot;actual&quot;]) . the monte carlo simulated samples are identical to the true distribution as we draw more and more samples. | . Example 2 . A binomial distribution of N (10) trails where the probability of success follows a beta distribution (2, 2). . This quite happens in bayesian modelling where we need to estimate the mean of joint probability of prior and likelihood (we will discuss in detail in some other blog post). In this example $$ y| phi = Bin(N, phi) $$ where $$ phi = beta(a, b) $$ . where N is the number of trails, a and b are shape parameter of beta distribution. . The joint distribution is defined as $$ P(y, phi) = P( phi) * p(y/ phi) $$ . Lets use monte carlo simulation to get the shape of this posterior distribution . beta = sps.beta.rvs(a=2, b=2, size=10000, random_state=1000) # calculating beta function with a and b as parameters new_range = np.arange(0, 100)/100 p_estimated = np.histogram(beta, bins=len(new_range))[0]/beta.shape[0] fig, ax = plt.subplots(figsize=(8*2,3.5), ncols=2, nrows=1) ax.flat[0].set(title=&#39;Beta Distribution n $a=2, b=2$&#39;) ax.flat[0].plot(new_range, p_estimated) ax.flat[1].set(title=&#39;Beta Distribution histogram n $a=2, b=2$&#39;) ax.flat[1].hist(beta) plt.show() # plt.legend(title=&quot;$alpha$&quot;, labels=alpha); . It follows the pattern similarly shown on wiki | Lets calculate y at all these phi parameters | . y = np.asarray([sps.binom.rvs(n=10, p=i, size=1, random_state=1000) for i in beta]) # Not sure how to vectorize this. . new_range = np.arange(0, int(y.max()+1)) p_estimated = np.histogram(y, bins=len(new_range))[0]/y.shape[0] fig, ax = plt.subplots(figsize=(8*2,3.5), ncols=2, nrows=1) ax.flat[0].set(title=&quot;Binomial distribution N(B, $ phi$) where $ phi$ = Beta(a, b)&quot;) ax.flat[0].plot(new_range, p_estimated) ax.flat[1].set(title=&#39;histogram&#39;) ax.flat[1].hist(y) plt.show() . This distribution is called beta-binomial | . print(f&quot;Estimated mean of the distribution: {np.mean(y)}&quot;) . Estimated mean of the distribution: 4.9688 . We can also calculate the probability of getting any value, lets calculate P(y=i| N, $ phi$), where i =1, 2, 3, 4,...10) | . for i in range(10): print(f&quot;P(y={i}|N, phi) = {(y==i).sum()/y.shape[0]}&quot;) . P(y=0|N, phi) = 0.0061 P(y=1|N, phi) = 0.0366 P(y=2|N, phi) = 0.0747 P(y=3|N, phi) = 0.1011 P(y=4|N, phi) = 0.1441 P(y=5|N, phi) = 0.2803 P(y=6|N, phi) = 0.1466 P(y=7|N, phi) = 0.1036 P(y=8|N, phi) = 0.0697 P(y=9|N, phi) = 0.0328 . In this tutorial, we understood how to calculate statistics for any kind of distribution using Monte carlo simulation. I will try to add more examples and different use cases of monte carlo simulation. Watch this space. | .",
            "url": "https://prakashjayy.github.io/carbon2silicon/monte-carlo/gamma%20distribution/beta%20distribution/beta-binomial%20distribution/scipy/matplotlib/python/2020/05/20/Monte-Carlo-estimation.html",
            "relUrl": "/monte-carlo/gamma%20distribution/beta%20distribution/beta-binomial%20distribution/scipy/matplotlib/python/2020/05/20/Monte-Carlo-estimation.html",
            "date": " • May 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://prakashjayy.github.io/carbon2silicon/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://prakashjayy.github.io/carbon2silicon/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}